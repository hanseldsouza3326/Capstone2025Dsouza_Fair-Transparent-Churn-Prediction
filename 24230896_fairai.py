# -*- coding: utf-8 -*-
"""24230896_FAIRAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B2dn-QeMjAPVYZpvRjksKDjXnxMMBDEC

# FAIRAI- Fair and Transparent Churn Prediction Through Mixture of Expert Based AI Model
This project introduces a Fair and Transparent Churn Prediction Through a Mixture of Expert-Based AI Model Framework that integrates Fairness Optimization, Explainability, Ethical AI Governance and Predective reinforcement modelling core into churn prediction. The framework uses specialized experts: genetic algorithms for bias mitigation, ANFIS and Fuzzy Logic for explainability, NLP for AI auditing, and reinforcement learning to balance ethical and financial outcomes. Our system leverages a real-world Irish financial data set and includes a dynamic Tableau dashboard for strategic decision-making. The result is a fairer, more interpretable, and ethically aligned AI-based financial decision-making system.

## Step 1: Data Understanding and Preprocessing
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
#Loading the dataset
file_path = "loan_data_set.csv"
loan_data = pd.read_csv(file_path)

#Printing the basic information for understanding
loan_data_info = loan_data.info()
loan_data_head = loan_data.head()
loan_data_info, loan_data_head

# Handling Missing Values
loan_data.fillna({
    'Gender': loan_data['Gender'].mode()[0],
    'Married': loan_data['Married'].mode()[0],
    'Dependents': loan_data['Dependents'].mode()[0],
    'Self_Employed': loan_data['Self_Employed'].mode()[0],
    'Loan_Amount_Term': loan_data['Loan_Amount_Term'].mode()[0],
    'LoanAmount': loan_data['LoanAmount'].median(),
    'Credit_History': loan_data['Credit_History'].median()
}, inplace=True)

"""## Step 2: Encoding Categorical Features"""

# Label Encoding for binary categories
le = LabelEncoder()
loan_data['Gender'] = le.fit_transform(loan_data['Gender'])
loan_data['Married'] = le.fit_transform(loan_data['Married'])
loan_data['Education'] = le.fit_transform(loan_data['Education'])
loan_data['Self_Employed'] = le.fit_transform(loan_data['Self_Employed'])
loan_data['Loan_Status'] = le.fit_transform(loan_data['Loan_Status'])

# One-Hot Encoding for multi-category features
loan_data = pd.get_dummies(loan_data, columns=['Dependents', 'Property_Area'], drop_first=True)

# Drop the Loan_ID as it's not useful for modeling
loan_data.drop(columns=['Loan_ID'], inplace=True)

"""## Step 3: Feature Engineering"""

loan_data['Total_Income'] = loan_data['ApplicantIncome'] + loan_data['CoapplicantIncome']
loan_data['Income_to_Loan_Ratio'] = loan_data['Total_Income'] / loan_data['LoanAmount']
loan_data['EMI'] = (loan_data['LoanAmount'] * 1000) / loan_data['Loan_Amount_Term']
loan_data['Loan_Income_Percentage'] = loan_data['EMI'] / loan_data['Total_Income']

"""## Step 4: Outlier Detection and Handling"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing outliers using boxplots
numerical_cols = [
    'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
    'Total_Income', 'Income_to_Loan_Ratio', 'EMI', 'Loan_Income_Percentage'
]

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=loan_data[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

# Capping outliers
def cap_outliers(df, column):
    lower_percentile = df[column].quantile(0.05)
    upper_percentile = df[column].quantile(0.95)
    df[column] = df[column].clip(lower=lower_percentile, upper=upper_percentile)

# Apply to numerical columns
for col in numerical_cols:
    cap_outliers(loan_data, col)

"""## Step 5: Data Normalization"""

# Initialize the scaler
scaler = MinMaxScaler()

# Apply Min-Max scaling
loan_data[numerical_cols] = scaler.fit_transform(loan_data[numerical_cols])

loan_data.head()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 6))
sns.heatmap(loan_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix of Normalized Data")
plt.show()



from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import SimpleImputer
import pandas as pd

# Load the data
file_path = "loan_data_set.csv"
loan_data = pd.read_csv(file_path)

# Encode categorical variables and drop ID + target
X = pd.get_dummies(loan_data.drop(columns=['Loan_ID', 'Loan_Status']), drop_first=True)
y = loan_data['Loan_Status']

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, stratify=y, test_size=0.2, random_state=42)

# Train Logistic Regression
logreg = LogisticRegression(solver='saga', max_iter=2000)
logreg.fit(X_train, y_train)
y_pred_lr = logreg.predict(X_test)
y_proba_lr = logreg.predict_proba(X_test)

#Mixture-of-Experts (MoE)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import pandas as pd
import numpy as np

loan_data = pd.read_csv("loan_data_set.csv")
loan_data.dropna(inplace=True)
loan_data = loan_data.drop(columns=["Loan_ID"])

#One-hot encode categoricals
X = pd.get_dummies(loan_data.drop(columns=["Loan_Status"]), drop_first=True)
y = loan_data["Loan_Status"]

#Encode target
y = y.map({'Y': 1, 'N': 0})

# Impute and scale
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)

# Train expert models
expert_models = [
    LogisticRegression(max_iter=1000),
    RandomForestClassifier(),
    MLPClassifier(max_iter=500),
    SVC(probability=True)
]

for model in expert_models:
    model.fit(X_train, y_train)

# Collect expert predictions as input to the gate
expert_outputs_train = np.column_stack([model.predict(X_train) for model in expert_models])
expert_outputs_test = np.column_stack([model.predict(X_test) for model in expert_models])

# Gate model (e.g., decision tree)
gate = DecisionTreeClassifier(max_depth=3)
gate.fit(expert_outputs_train, y_train)

# Gate decides which expert to trust
final_preds = []
for i in range(len(X_test)):
    gate_input = expert_outputs_test[i].reshape(1, -1)
    expert_idx = gate.predict(gate_input)[0]
    expert_idx = min(expert_idx, len(expert_models) - 1)  # Ensure within bounds
    final_pred = expert_models[expert_idx].predict(X_test[i].reshape(1, -1))[0]
    final_preds.append(final_pred)

# Evaluate
accuracy = accuracy_score(y_test, final_preds)
print("Custom MoE Accuracy:", accuracy)

#Evaluation Metrics
!pip install aif360 --quiet

from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

# Ensure your final MoE predictions are assigned
y_pred_moe = final_preds  # final_preds is generated in your custom MoE loop

# Convert to numpy if needed
import numpy as np
y_pred_moe = np.array(y_pred_moe)

# Core performance metrics
accuracy = accuracy_score(y_test, y_pred_moe)
recall = recall_score(y_test, y_pred_moe)
f1 = f1_score(y_test, y_pred_moe)
roc_auc = roc_auc_score(y_test, y_pred_moe)

print("üìä Evaluation Metrics for Custom Mixture of Experts")
print(f"Accuracy     : {accuracy:.4f}")
print(f"Recall       : {recall:.4f}")
print(f"F1 Score     : {f1:.4f}")
print(f"ROC AUC      : {roc_auc:.4f}")

# confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_moe))

# classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_moe))

from aif360.sklearn.metrics import disparate_impact_ratio

#Gender column assumed to exist in original loan_data
prot_attr = loan_data.loc[y_test.index, 'Gender'].map({'Male': 1, 'Female': 0})
di = disparate_impact_ratio(y_true=y_test, y_pred=y_pred_moe, prot_attr=prot_attr, priv_group=1, pos_label=1)
print(f"\n  Disparate Impact (Gender): {di:.4f}")

# ‚úÖ Install ELI5 (if not already installed)
!pip install eli5 --quiet

# ‚úÖ Imports
import pandas as pd
import eli5
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# ‚úÖ Load data
loan_data = pd.read_csv("loan_data_set.csv")

# ‚úÖ Safe drop: Only drop Loan_ID if it exists
if "Loan_ID" in loan_data.columns:
    loan_data.drop(columns=["Loan_ID"], inplace=True)

# ‚úÖ Drop rows where target is missing
loan_data.dropna(subset=["Loan_Status"], inplace=True)

# ‚úÖ Encode target: Y ‚Üí 1, N ‚Üí 0
loan_data["Loan_Status"] = loan_data["Loan_Status"].map({"Y": 1, "N": 0})

# ‚úÖ One-hot encode features
X = pd.get_dummies(loan_data.drop(columns=["Loan_Status"]), drop_first=True)
y = loan_data["Loan_Status"]

# ‚úÖ Save column names for explainability
columns = X.columns

# ‚úÖ Impute missing values
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X)

# ‚úÖ Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# ‚úÖ Convert back to DataFrame for explainability
X_scaled_df = pd.DataFrame(X_scaled, columns=columns)

# ‚úÖ Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_df, y, stratify=y, test_size=0.2, random_state=42
)

# ‚úÖ Logistic Regression Model
logreg = LogisticRegression(max_iter=2000)
logreg.fit(X_train, y_train)

# ‚úÖ Global Explainability (Feature Importance)
eli5.show_weights(logreg, feature_names=X_train.columns.tolist())

# ‚úÖ Local Explainability (One test instance)
eli5.show_prediction(logreg, X_test.iloc[0], feature_names=X_train.columns.tolist())

"""## Step : Original- Data Level Ethical Audit ( To check if the dataset itself has any ethical concerns)"""

#Ethical Governance Check
loan_data = pd.read_csv("loan_data_set.csv")
loan_data.drop(columns=["Loan_ID"], inplace=True)
loan_data.dropna(subset=["Loan_Status"], inplace=True)
loan_data["Loan_Status"] = loan_data["Loan_Status"].map({'Y': 1, 'N': 0})

#Creating EMI and Loan_Income_Percentage
loan_data["EMI"] = (loan_data["LoanAmount"] * 1000) / loan_data["Loan_Amount_Term"]
loan_data["Total_Income"] = loan_data["ApplicantIncome"] + loan_data["CoapplicantIncome"]
loan_data["Loan_Income_Percentage"] = loan_data["EMI"] / loan_data["Total_Income"]

#Initialize audit log column
loan_data["Ethical_Audit_Result"] = "Compliant"

#Applying Ethical Governance Rules
for i in loan_data.index:
    reasons = []

    # Rule 1: Rejected despite low EMI and good credit
    if loan_data.loc[i, "Credit_History"] == 1 and loan_data.loc[i, "EMI"] < 0.3 and loan_data.loc[i, "Loan_Status"] == 0:
        reasons.append("Rejected despite good credit and low EMI")

    # Rule 2: Gender bias flag ‚Äî if female with good credit but rejected
    if "Gender" in loan_data.columns and loan_data.loc[i, "Gender"] == "Female" and loan_data.loc[i, "Credit_History"] == 1 and loan_data.loc[i, "Loan_Status"] == 0:
        reasons.append("Possible gender bias")

    # Rule 3: Approved despite high EMI and poor credit
    if loan_data.loc[i, "Credit_History"] == 0 and loan_data.loc[i, "Loan_Status"] == 1 and loan_data.loc[i, "Loan_Income_Percentage"] > 0.6:
        reasons.append("Approved despite high EMI and poor credit")

    # Combine results:
    if reasons:
        loan_data.loc[i, "Ethical_Audit_Result"] = "Flagged: " + "; ".join(reasons)

#Exporting audit results
audit_filename = "ethical_audit_log.csv"
loan_data.to_csv(audit_filename, index=False)

#Summary stats
flagged = loan_data[loan_data["Ethical_Audit_Result"].str.contains("Flagged")]
compliance_rate = (loan_data["Ethical_Audit_Result"] == "Compliant").mean()

print(f"‚úÖ Audit complete. Compliance rate: {compliance_rate:.2%}")
print(f"üö® Flagged decisions: {len(flagged)}")
print(f"üìÑ CSV exported: {audit_filename}")
flagged.head()

# Identify target variable
y = loan_data["Loan_Status"]

# Drop target temporarily to one-hot encode features
X = loan_data.drop("Loan_Status", axis=1)

# ‚úÖ One-hot encode all categorical variables
X = pd.get_dummies(X, drop_first=True)

# ‚úÖ Proceed with imputation and scaling
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

numeric_cols = X.select_dtypes(include=["int64", "float64"]).columns
imputer = SimpleImputer(strategy="mean")
scaler = StandardScaler()

X_imputed = imputer.fit_transform(X[numeric_cols])
X_scaled = scaler.fit_transform(X_imputed)

# ‚úÖ Convert back to DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=numeric_cols)

# ‚úÖ Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_df, y, test_size=0.2, stratify=y, random_state=42
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix

# ‚úÖ Step 1: Train Logistic Regression
baseline_model = LogisticRegression(max_iter=2000, solver='lbfgs')
baseline_model.fit(X_train, y_train)

# ‚úÖ Step 2: Predict
y_pred_baseline = baseline_model.predict(X_test)
y_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]

# ‚úÖ Step 3: Evaluate
print("üîπ Logistic Regression (Baseline Model) Performance üîπ")
print("Accuracy:", accuracy_score(y_test, y_pred_baseline))
print("F1 Score:", f1_score(y_test, y_pred_baseline))
print("ROC AUC:", roc_auc_score(y_test, y_proba_baseline))
print("\nClassification Report:\n", classification_report(y_test, y_pred_baseline))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_baseline))

from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import numpy as np

# ---- Step 1: Define Expert Models ----
expert_models = {
    'LR': LogisticRegression(max_iter=2000),
    'RF': RandomForestClassifier(n_estimators=100, random_state=42),
    'MLP': MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=42),
    'SVC': SVC(probability=True)
}

# ---- Step 2: Train All Experts ----
expert_predictions_train = []
expert_predictions_test = []
for name, model in expert_models.items():
    model.fit(X_train, y_train)
    preds_train = model.predict(X_train)
    preds_test = model.predict(X_test)

    expert_predictions_train.append(preds_train)
    expert_predictions_test.append(preds_test)

# ---- Step 3: Train Gating Model ----
# Gating model learns which expert predicts best for each training sample
best_expert_indices = np.argmin([
    (preds_train != y_train.values).astype(int) for preds_train in expert_predictions_train
], axis=0)

X_gate_train = X_train.copy()
y_gate_train = best_expert_indices

gating_model = DecisionTreeClassifier(max_depth=3, random_state=42)
gating_model.fit(X_gate_train, y_gate_train)

# ---- Step 4: Use Gate to Select Expert for Each Test Instance ----
gate_choices = gating_model.predict(X_test)
final_predictions = []

for i, choice in enumerate(gate_choices):
    pred = expert_predictions_test[choice][i]
    final_predictions.append(pred)

# ---- Step 5: Evaluate MoE Output ----
print("\nüîπ Mixture of Experts (MoE) Performance üîπ")
print("Accuracy:", accuracy_score(y_test, final_predictions))
print("F1 Score:", f1_score(y_test, final_predictions))
print("ROC AUC:", roc_auc_score(y_test, final_predictions))

# Baseline metrics
baseline_acc = accuracy_score(y_test, y_pred_baseline)
baseline_f1 = f1_score(y_test, y_pred_baseline)
baseline_auc = roc_auc_score(y_test, y_proba_baseline)

# MoE metrics (from previous step)
moe_acc = accuracy_score(y_test, final_predictions)
moe_f1 = f1_score(y_test, final_predictions)
moe_auc = roc_auc_score(y_test, final_predictions)

# Display Comparison
print("üîÅ Baseline vs MoE Comparison")
print(f"Baseline - Accuracy: {baseline_acc:.2f}, F1: {baseline_f1:.2f}, ROC AUC: {baseline_auc:.2f}")
print(f"MoE      - Accuracy: {moe_acc:.2f}, F1: {moe_f1:.2f}, ROC AUC: {moe_auc:.2f}")

# ‚úÖ Import again if needed
import pandas as pd
import numpy as np

# ‚úÖ Reconstruct test set indices and merge predictions with original gender
results_df = pd.DataFrame(X_test.copy())
results_df['actual'] = y_test.values
results_df['predicted'] = final_predictions

# ‚úÖ Pull Gender column from original loan_data using index
# (Make sure loan_data has Gender column as 'Male' or 'Female')
results_df['Gender'] = loan_data.loc[X_test.index, 'Gender'].map({'Male': 1, 'Female': 0})

# ‚úÖ Split by Gender: 0 = Female, 1 = Male
group_female = results_df[results_df['Gender'] == 0]
group_male = results_df[results_df['Gender'] == 1]

# ‚úÖ Approval rate = predicted == 1 (approved)
approval_rate_female = group_female['predicted'].mean()
approval_rate_male = group_male['predicted'].mean()

# ‚úÖ Calculate Disparate Impact
disparate_impact = approval_rate_female / approval_rate_male

# ‚úÖ Display Results
print("‚öñÔ∏è Fairness Analysis (Disparate Impact - Gender)")
print(f"Approval Rate (Female): {approval_rate_female:.2f}")
print(f"Approval Rate (Male):   {approval_rate_male:.2f}")
print(f"Disparate Impact (Female / Male): {disparate_impact:.2f}")

# ‚úÖ Optional interpretation
if 0.8 <= disparate_impact <= 1.25:
    print("‚úÖ Fairness: Disparate Impact is within acceptable bounds (80% rule).")
else:
    print("‚ö†Ô∏è Potential fairness concern: Disparate Impact outside acceptable range.")

# 1. Prepare data from scratch (imputation + scaling + column names preserved)
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Get your original preprocessed DataFrame with all dummies
X = pd.get_dummies(loan_data.drop(columns=["Loan_Status"]), drop_first=True)
y = loan_data["Loan_Status"]

# Save column names
feature_names = X.columns.tolist()

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Rebuild DataFrame with same feature names
X_final = pd.DataFrame(X_scaled, columns=feature_names)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, stratify=y, random_state=42)

# Retrain the model on this corrected data
logreg = LogisticRegression(max_iter=2000)
logreg.fit(X_train, y_train)

# 2. ‚úÖ Now ELI5 will work
import eli5
from IPython.display import display

display(eli5.show_weights(logreg, feature_names=feature_names))
display(eli5.show_prediction(logreg, X_test.iloc[0], feature_names=feature_names))

# ‚úÖ Load original loan data
raw_data = pd.read_csv("loan_data_set.csv")

# ‚úÖ Ensure correct index alignment
raw_data = raw_data.loc[X_test.index].copy()

# ‚úÖ Clean and prepare for audit
raw_data["Loan_Status"] = raw_data["Loan_Status"].map({"Y": 1, "N": 0})

# ‚úÖ Add model predictions
raw_data["Predicted"] = final_predictions

# ‚úÖ Add engineered features
raw_data["Total_Income"] = raw_data["ApplicantIncome"] + raw_data["CoapplicantIncome"]
raw_data["EMI"] = (raw_data["LoanAmount"] * 1000) / raw_data["Loan_Amount_Term"]
raw_data["Loan_Income_Percentage"] = raw_data["EMI"] / raw_data["Total_Income"]

# ‚úÖ Initialize audit column
raw_data["Ethical_Audit_Flag"] = "Compliant"

# ---- Rule 1: Rejected despite low EMI and good credit ----
mask1 = (raw_data["Credit_History"] == 1) & (raw_data["EMI"] < 0.3) & (raw_data["Predicted"] == 0)
raw_data.loc[mask1, "Ethical_Audit_Flag"] = "Flag: Rejected w/ Good Credit & Low EMI"

# ---- Rule 2: Approved with high EMI and poor credit ----
mask2 = (raw_data["Credit_History"] == 0) & (raw_data["Loan_Income_Percentage"] > 0.6) & (raw_data["Predicted"] == 1)
raw_data.loc[mask2, "Ethical_Audit_Flag"] = "Flag: Approved w/ Poor Credit & High EMI"

# ---- Rule 3: Optional gender bias rule ----
if "Gender" in raw_data.columns:
    mask3 = (raw_data["Gender"] == "Female") & (raw_data["Credit_History"] == 1) & (raw_data["Predicted"] == 0)
    raw_data.loc[mask3, "Ethical_Audit_Flag"] = "Flag: Female Rejected w/ Good Credit"

# ‚úÖ Save audit results
raw_data.to_csv("ethical_audit_results.csv", index=False)

# ‚úÖ Summary
print("üîç Ethical Audit Summary:")
print(raw_data["Ethical_Audit_Flag"].value_counts())

import numpy as np
import random

# Step 1: Prepare RL training data from audit log
df_rl = pd.read_csv("ethical_audit_results.csv")

# Step 2: Define state variables (features)
state_cols = ["Credit_History", "EMI", "Loan_Income_Percentage"]
states = df_rl[state_cols].values

# Step 3: Define actions and reward structure
actions = [0, 1]  # 0: Reject, 1: Approve
q_table = np.zeros((len(states), len(actions)))

# Step 4: Reward logic based on outcome
def compute_reward(row, action):
    if row["Ethical_Audit_Flag"] != "Compliant":
        return -10  # ethical violation
    if action == row["Loan_Status"]:
        return 10   # correct prediction
    else:
        return -5   # incorrect prediction

# Step 5: Q-learning loop (simple simulation)
alpha = 0.1  # learning rate
gamma = 0.9  # discount
episodes = 1000

for episode in range(episodes):
    for i in range(len(states)):
        s = i
        action = random.choice(actions)
        reward = compute_reward(df_rl.iloc[i], action)
        best_next = np.max(q_table[s])
        q_table[s, action] += alpha * (reward + gamma * best_next - q_table[s, action])

print("‚úÖ RL policy learned (simulated). Final Q-values:")
print(q_table[:5])  # show sample

